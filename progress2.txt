# Bug Fix Progress Log
Created: Wed Jan 14 22:59:55 +04 2026

[2026-01-14] BUG-001 - COMPLETED
Status: Fixed async event loop error in power sample callback
Description: Fixed 'no running event loop' errors in WebSocket callbacks by capturing the main event loop at the start of profiled_generate() and replacing all asyncio.create_task() calls with asyncio.run_coroutine_threadsafe() to safely broadcast messages from background threads. Updated 6 callbacks: stream_power_sample, stream_section_event, stream_token_complete, stream_layer_metrics, stream_component_metrics, and stream_inference_complete.
Files Modified: backend/main.py

[2026-01-14] BUG-002 - COMPLETED
Status: Fixed StableLM model compatibility with TextIteratorStreamer
Description: Added StableLM architecture detection in model_detector.py with _is_stablelm_structure() and _detect_stablelm() methods. Created is_streaming_compatible() function to identify models incompatible with TextIteratorStreamer. Modified backend/main.py profiled_generate endpoint to check streaming compatibility and use non-streaming generation (use_cache=False) for incompatible models like StableLM. This prevents crashes from KV cache issues while still allowing profiling to complete successfully with appropriate warnings.
Files Modified: backend/profiling/model_detector.py, backend/main.py

[2026-01-14] BUG-003 - COMPLETED
Status: Fixed section event callbacks async issue
Description: All callback functions (stream_section_event, stream_token_complete, stream_layer_metrics, stream_component_metrics, stream_inference_complete) were already fixed as part of BUG-001. They all use asyncio.run_coroutine_threadsafe() with main_loop for thread-safe WebSocket broadcasting. No additional changes needed.
Files Modified: backend/main.py (already fixed in BUG-001)

[2026-01-14] BUG-004 - COMPLETED
Status: Fixed token timing measurement
Description: Fixed token duration measurement which was timing the append() operation (~0ms) instead of actual token generation time. Implemented proper timing by tracking time between consecutive token arrivals from the streamer. Added last_token_time variable to measure duration as (current_token_time - last_token_time). For the first token, uses a reasonable default of 50ms. This fix ensures accurate token duration values (10-100ms per token) and correct energy calculations.
Files Modified: backend/main.py

[2026-01-14] BUG-005 - COMPLETED
Status: Fixed token field name mismatch between backend and frontend
Description: Fixed field name inconsistency where backend sent 'token_index' but frontend expected 'token_position'. Updated three WebSocket callbacks in backend/main.py: stream_token_complete, stream_layer_metrics, and stream_component_metrics to send 'token_position' instead of 'token_index'. This ensures token position displays correctly in all frontend components (TokenGenerationStream hover tooltips, LiveLayerHeatmap, CurrentOperationIndicator, etc.).
Files Modified: backend/main.py

[2026-01-14] BUG-007 - COMPLETED
Status: Added StableLM architecture detection in model_detector.py
Description: StableLM architecture detection was already added as part of BUG-002 fix. The model_detector.py now includes _is_stablelm_structure() method to properly detect StableLM models and _detect_stablelm() method to return correct component paths. StableLM is now correctly identified as 'stablelm' instead of being misidentified as 'llama', ensuring correct profiling behavior.
Files Modified: backend/profiling/model_detector.py (already fixed in BUG-002)

[2026-01-14] BUG-008 - COMPLETED
Status: Fixed energy calculation using near-zero duration
Description: Energy calculation was automatically fixed by BUG-004. Once token timing measurement was corrected to track actual token generation time (10-100ms per token) instead of append() operation time (~0ms), the energy calculation (current_power_mw * token_duration_ms / 1000.0) now produces realistic token energy values (0.1-10 mJ per token depending on model and hardware).
Files Modified: backend/main.py (already fixed in BUG-004)

[2026-01-14] BUG-009 - COMPLETED
Status: Added model streaming compatibility check
Description: Streaming compatibility check was already added as part of BUG-002 fix. The is_streaming_compatible() function in model_detector.py checks if a model supports TextIteratorStreamer by detecting known incompatible architectures (like StableLM). The profiled_generate endpoint in backend/main.py checks compatibility before using streaming and falls back to non-streaming generation for incompatible models.
Files Modified: backend/profiling/model_detector.py, backend/main.py (already fixed in BUG-002)

[2026-01-14] BUG-016 - COMPLETED
Status: Added Gemma 3 architecture detection
Description: Gemma architecture detection was already implemented in model_detector.py. The detector includes _is_gemma_structure() method and _detect_gemma() method that properly identifies Gemma models (including Gemma 2 and Gemma 3) by checking for model_type == 'gemma' or 'gemma2'. Returns correct component paths with RMSNorm and standard Llama-like structure.
Files Modified: backend/profiling/model_detector.py (already implemented)

[2026-01-14] BUG-017 - COMPLETED
Status: Fixed layer profiler hooks for different architectures
Description: Enhanced LayerProfiler in layer_profiler.py to better support multiple architectures. Added registration statistics tracking to count successful hook registrations per component type. Improved logging to show architecture name, registration summary, and warnings when major components aren't found. Added diagnostic logging for first layer structure. Enhanced error messages to only log on first layer (avoid spam) and include architecture information. Added get_registration_summary() method for external code to check registration success. The LayerProfiler already uses ModelArchitectureDetector dynamically, so it now properly handles all supported architectures (Llama, Mistral, Phi, Qwen, Gemma, StableLM) with better diagnostics when hooks fail to attach.
Files Modified: backend/profiling/layer_profiler.py

[2026-01-14] BUG-006 - COMPLETED
Status: Fixed WebSocket URL hardcoding in frontend
Description: Fixed hardcoded localhost:8000 URLs in frontend by creating centralized configuration. Created src/lib/config.ts that exports API_BASE_URL and getWebSocketUrl() function, both of which check environment variables (NEXT_PUBLIC_API_URL and NEXT_PUBLIC_WS_URL) before falling back to localhost:8000. Updated src/lib/api.ts to import API_BASE_URL from config. Updated src/components/profiling/ProfilingControls.tsx to use API_BASE_URL for model fetching instead of hardcoded URL. Updated src/lib/websocket.ts and src/lib/profilingWebsocket.ts to use getWebSocketUrl() for WebSocket connections. The WebSocket URL is automatically derived from the HTTP API URL (http:// -> ws://, https:// -> wss://), but can be overridden with NEXT_PUBLIC_WS_URL. Frontend now supports deployment to any URL by setting NEXT_PUBLIC_API_URL environment variable.
Files Modified: src/lib/config.ts (new), src/lib/api.ts, src/components/profiling/ProfilingControls.tsx, src/lib/websocket.ts, src/lib/profilingWebsocket.ts

[2026-01-14] BUG-010 - COMPLETED
Status: Fixed profiling cleanup on error
Description: Fixed potential cleanup issues when profiling fails mid-execution. The InferencePipelineProfiler context manager already had proper cleanup in its finally block (lines 339-387 in pipeline_profiler.py), which handles stopping power monitoring, detaching layer profiler hooks, unpatching deep profiler, and saving data to database. However, backend/main.py had redundant cleanup code (lines 2157-2168) that was removed since the context manager handles it automatically. Added proper error handling in the except block to send profiling_error WebSocket events to the frontend when exceptions occur, using the existing ProfilingMessageType.ERROR message type. This ensures the frontend is notified of errors and all resources (PowerMonitor subprocess, layer hooks, deep profiler patches) are properly cleaned up even when profiling fails. Added import time statement to support error timestamp generation.
Files Modified: backend/main.py

[2026-01-14] BUG-013 - COMPLETED
Status: Fixed power sample timestamp using wrong base
Description: Fixed power sample timestamps in WebSocket events to use relative time instead of Unix timestamps. The backend was sending sample.timestamp (Unix timestamp) but the frontend PowerTimeSeriesChart expects timestamp in milliseconds relative to profiling start. Updated stream_power_sample callback in backend/main.py to send sample.relative_time_ms as the timestamp field in both the message and data objects. Also added the phase field to the data object for proper phase display in charts. This fix ensures the power chart X-axis shows time from 0 to profiling duration in seconds, rather than showing huge Unix timestamp values.
Files Modified: backend/main.py

[2026-01-14] BUG-014 - COMPLETED
Status: Fix PowerTimeSeriesChart data format mismatch (verified already fixed)
Description: After thorough code review, confirmed that the PowerTimeSeriesChart data format is already correct and matching between backend and frontend. Backend sends PowerSample via WebSocket with all required fields: timestamp (relative_time_ms), cpu_power_mw, gpu_power_mw, ane_power_mw, dram_power_mw, total_power_mw, phase. Frontend PowerSample TypeScript interface expects exactly these fields. ProfilingContext correctly extracts message.data and adds to powerSamples array. All types validate correctly with TypeScript compilation succeeding. The data flow from backend -> WebSocket -> frontend is fully aligned. This bug was likely fixed as a side effect of BUG-013 (timestamp fix).
Files Modified: None (already fixed)

[2026-01-14] BUG-011 - COMPLETED
Status: Fixed WebSocket reconnection race condition
Description: Fixed race condition in profilingWebsocket.ts reconnection logic that could cause duplicate connections during rapid connect/disconnect cycles. Added 'disconnecting' state to ConnectionState type to track when disconnect is in progress. Enhanced connect() method to: (1) check connectionState instead of just ws.readyState to prevent new connections during disconnecting, (2) cancel any pending reconnection timer before connecting to prevent duplicate scheduled reconnections. Enhanced scheduleReconnect() to: (1) prevent reconnection if already connecting/connected, (2) prevent scheduling multiple reconnection attempts by returning early if reconnectTimer already exists, (3) clear reconnectTimer reference after timeout fires. Enhanced disconnect() to: (1) set 'disconnecting' state before closing WebSocket to prevent new connections, (2) return early if already disconnected to avoid redundant work. These changes ensure exactly one active WebSocket connection exists during rapid connect/disconnect cycles and prevent orphaned reconnection timers.
Files Modified: src/lib/profilingWebsocket.ts

[2026-01-14] BUG-020 - COMPLETED
Status: Fixed LiveLayerHeatmap not receiving data
Description: Fixed LiveLayerHeatmap showing "Waiting for token generation..." by adding full layer-by-layer metrics to token_complete WebSocket events. The issue was that backend only sent layer_metrics_summary (aggregated stats) but LiveLayerHeatmap expected full LayerMetrics[] array with per-layer component data. Modified backend/profiling/pipeline_profiler.py emit_token_complete_event() to build layer_metrics array by grouping ComponentTiming data by layer_idx, creating proper LayerMetrics structure with layer_index and components array containing component_name, duration_ms, activation_mean/std/max, and sparsity for each component. Updated backend/main.py stream_token_complete callback to include layer_metrics field in WebSocket message. Updated frontend src/types/index.ts TokenCompleteMessage interface to include optional layer_metrics field. Updated src/components/profiling/ProfilingContext.tsx to map message.layer_metrics to token.layers instead of hardcoded empty array. Now LiveLayerHeatmap receives full layer data and displays real-time heatmap visualization showing component activity (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj, layer norms) across all layers for each generated token.
Files Modified: backend/profiling/pipeline_profiler.py, backend/main.py, src/types/index.ts, src/components/profiling/ProfilingContext.tsx

[2026-01-14] BUG-015 - COMPLETED
Status: Fixed ProfilingContext token mapping
Description: Fixed field name mismatches between backend and frontend for token_complete WebSocket events. Backend was sending 'token_index' but frontend expected 'token_position', and backend was sending 'avg_power_mw' but frontend expected 'power_snapshot_mw'. Updated backend/profiling/pipeline_profiler.py emit_token_complete_event() to send token_position instead of token_index and power_snapshot_mw instead of avg_power_mw in the token_data dictionary. This ensures all TokenMetrics fields are properly populated in the frontend, fixing undefined values in hover tooltips, React DevTools, and any component using token metrics. Now all token data fields (token_position, token_text, duration_ms, energy_mj, power_snapshot_mw, layer_metrics) map correctly between backend and frontend.
Files Modified: backend/profiling/pipeline_profiler.py

[2026-01-14] BUG-018 - COMPLETED
Status: Fixed deep profiler attention metrics extraction
Description: Fixed DeepAttentionProfiler to handle different model architectures that return attention weights in different tuple positions or formats. The original code assumed attention weights were always at result[1], but different architectures (StableLM, Gemma, etc.) return different tuple formats: (output, attention_weights), (output, attention_weights, cache), or other variations. Implemented robust extraction by iterating through all tuple elements (skipping first element which is always output) and checking if each item is a torch.Tensor with dimensions matching attention weight patterns (3D or 4D tensors). Added this fix to both _create_instrumented_forward() and _create_detailed_instrumented_forward() methods. Enhanced error handling to silently skip attention metrics computation when weights aren't available or have unexpected format, allowing profiling to continue gracefully for all architectures. Now deep profiling shows attention metrics (entropy, sparsity, max weights) for supported architectures and gracefully skips them for architectures where attention weights aren't accessible, without crashing or producing errors.
Files Modified: backend/profiling/deep_profiler.py

[2026-01-14] BUG-012 - COMPLETED
Status: Added error boundary for profiling components
Description: Created ProfilingErrorBoundary React component to catch and handle JavaScript errors in profiling UI components, preventing entire dashboard crashes. The error boundary displays a user-friendly error UI with: (1) clear error icon and explanation of possible causes (invalid data, rendering issues, WebSocket problems, browser compatibility), (2) error details panel visible in development mode showing error message and component stack trace for debugging, (3) "Try Again" button to reset error state and re-render components, (4) "Reload Page" button for complete page refresh. Wrapped RealTimeView and HistoryBrowser lazy-loaded components in EnergyProfilerPanel.tsx with the error boundary. JavaScript errors in profiling components now show graceful error UI instead of crashing the entire page, improving user experience and making debugging easier in development mode.
Files Modified: src/components/profiling/ProfilingErrorBoundary.tsx (new), src/components/profiling/EnergyProfilerPanel.tsx

[2026-01-14] BUG-019 - COMPLETED
Status: Added prefill vs decode phase energy tracking
Description: Enhanced phase-based energy tracking by adding power_monitor.set_phase() calls in the section context manager. Modified backend/profiling/pipeline_profiler.py _session_section() to call power_monitor.set_phase(phase) when each section starts, ensuring power samples are correctly tagged with their inference phase (idle, pre_inference, prefill, decode, post_inference). The existing energy calculation code already sums section energies by phase to compute prefill_energy_mj and decode_energy_mj, which are stored in the database via update_run_metrics(). With this fix, power samples collected during prefill sections are tagged with phase='prefill' and samples during decode sections are tagged with phase='decode', enabling accurate per-phase energy analysis. Run summaries now show accurate prefill_energy_mj and decode_energy_mj values for detailed energy breakdown analysis.
Files Modified: backend/profiling/pipeline_profiler.py

[2026-01-14] BUG-021 - COMPLETED
Status: Fixed CurrentOperationIndicator not updating
Description: Fixed CurrentOperationIndicator component not receiving WebSocket section_start and section_end events due to message structure mismatch. The backend sends these events with phase and section_name nested inside a 'data' object, but the frontend TypeScript interfaces expected them as direct message properties. Updated src/types/index.ts to wrap SectionStartMessage and SectionEndMessage data fields in a 'data' object matching the backend structure. Updated src/components/profiling/ProfilingContext.tsx section_start and section_end handlers to access message.data.phase, message.data.section_name, message.data.duration_ms, and message.data.energy_mj instead of accessing these fields directly on the message. This fix ensures CurrentOperationIndicator receives proper phase and section updates, displaying transitions through idle -> pre_inference -> prefill -> decode -> post_inference phases during profiling, with the phase-specific icons, colors, and progress indicators working correctly.
Files Modified: src/types/index.ts, src/components/profiling/ProfilingContext.tsx

[2026-01-14] BUG-024 - COMPLETED
Status: Added input token count tracking
Description: Fixed missing input token count tracking for accurate per-token energy analysis. Added input_token_count and output_token_count fields to ProfilingSession dataclass in backend/profiling/pipeline_profiler.py. Modified backend/main.py profiled_generate endpoint to calculate input_token_count after tokenization (len(inputs['input_ids'][0])) and store it in session.input_token_count. Added output_token_count tracking in both streaming mode (len(generated_tokens)) and non-streaming mode (num_tokens) and store in session.output_token_count. Updated _save_run_to_database() in pipeline_profiler.py to use session.input_token_count and session.output_token_count instead of placeholder values. Simplified the energy calculation loop to only sum energies by phase, removing the incorrect token counting logic that was setting input_token_count to 1. Now profiling runs have accurate input_token_count and output_token_count populated in the database, enabling proper calculation of energy_per_input_token_mj and energy_per_output_token_mj for detailed per-token energy efficiency analysis.
Files Modified: backend/profiling/pipeline_profiler.py, backend/main.py

[2026-01-15] BUG-025 - COMPLETED
Status: Fixed model dropdown not showing all models
Description: Fixed model dropdown in ProfilingControls to properly show all valid trained models and support refreshing the list without page reload. Enhanced backend/main.py discover_model_directories() function to filter model directories in model_output path by requiring config.json file (previously included all subdirectories). This ensures only valid model directories appear in the dropdown. Refactored frontend src/components/profiling/ProfilingControls.tsx to extract fetchModels() function outside useEffect, enabling it to be called on-demand. Added "Refresh" button next to the Model label that calls fetchModels() to re-fetch the model list from the backend. The Refresh button shows "Refreshing..." text while loading and is disabled during profiling or when already loading. Backend already scans model directories on each /api/models request, so new models added to the models directory now appear in the dropdown after clicking Refresh, without requiring server restart or page reload. Frontend fetchModels() updated to only set default model path if no model is currently selected, preventing unintended model changes during refresh.
Files Modified: backend/main.py, src/components/profiling/ProfilingControls.tsx

[2026-01-15] BUG-022 - COMPLETED
Status: Added model loading progress indication
Description: Fixed missing user feedback during model loading by adding WebSocket progress events. Added MODEL_LOADING message type to backend ProfilingMessageType enum and frontend ProfilingMessageType union in src/types/index.ts. Created ModelLoadingMessage interface with status (loading | complete | error), model_name, model_path, and message fields. Modified backend/main.py profiled_generate endpoint to broadcast model_loading events at four key stages: (1) before loading tokenizer, (2) before loading model weights, (3) before moving model to device, and (4) after model is ready. Each event includes the model name and descriptive progress message. Enhanced frontend src/components/profiling/ProfilingContext.tsx by adding isLoadingModel and modelLoadingMessage state fields, and subscribing to model_loading events to update these states. Modified src/components/profiling/ProfilingControls.tsx to read isLoadingModel and modelLoadingMessage from context and display the loading message in the status indicator with yellow color when model is loading. Updated src/lib/profilingWebsocket.ts to add model_loading handler type to ProfilingEventHandlers and import ModelLoadingMessage type. Now users see clear progress indication during model loading instead of the app appearing frozen during the 10-60 second loading period.
Files Modified: backend/main.py, src/types/index.ts, src/components/profiling/ProfilingContext.tsx, src/components/profiling/ProfilingControls.tsx, src/lib/profilingWebsocket.ts

[2026-01-15] BUG-023 - COMPLETED
Status: Fixed database path resolution
Description: Fixed ProfileDatabase using relative path 'backend/profiling.db' which could fail when server is started from different working directories. Modified backend/profiling/database.py __init__ method to accept optional db_path parameter (previously required string with default value). When db_path is None, the constructor now: (1) checks DATABASE_PATH environment variable first, (2) if not set, constructs absolute path by resolving backend directory location using Path(__file__).resolve().parent.parent and appending 'profiling.db'. Updated connect() method to log absolute path instead of relative path for clarity when showing database location. Updated init_database() helper function signature to also accept Optional[str] instead of string with default. Added os import for environment variable support. Database now uses consistent absolute path (/Users/miguelitodeguzman/projects/sdb/ml-dashboard/backend/profiling.db) regardless of working directory, and supports DATABASE_PATH environment variable for custom locations. Verified fix works correctly from different working directories using Python tests.
Files Modified: backend/profiling/database.py

[2026-01-15] BUG-027 - COMPLETED
Status: Fixed plist buffer parsing edge cases
Description: Fixed PowerMonitor plist parsing to handle edge cases where plist output is split across buffer boundaries or contains unexpected XML. Added _plist_buffer_start_time field to track buffer age and implement 10-second timeout for incomplete plists to prevent unbounded buffering. Enhanced _sampling_loop() method with robust XML boundary detection: (1) added buffer timeout check that discards incomplete samples after 10 seconds with warning message, (2) improved plist start detection to handle cases where XML tags are split across lines, (3) added verification that complete buffer has proper XML structure (checks for both start tag and end tag) before parsing, (4) added explicit logging for malformed buffers missing start tags. Enhanced error handling to distinguish between InvalidFileException and other parsing errors with separate warning messages. Reset buffer state (_plist_buffer and _plist_buffer_start_time) in all cleanup paths including timeout, malformed buffer, and successful parsing. This fix ensures extended profiling runs have consistent power sample collection without parse errors, gracefully skipping malformed samples while preventing memory issues from unbounded buffering.
Files Modified: backend/profiling/power_monitor.py

[2026-01-15] BUG-031 - COMPLETED
Status: Fixed temperature and top_p not being applied
Description: Fixed temperature and top_p generation parameters not being applied to model generation. Temperature was already present in ProfiledGenerateRequest but top_p was hardcoded to 0.9. Added top_p field (default 0.9) to backend/main.py ProfiledGenerateRequest Pydantic model. Updated both streaming and non-streaming generation_kwargs in profiled_generate endpoint to use request.top_p instead of hardcoded value. Added top_p field to frontend src/types/index.ts ProfiledGenerateRequest interface. Added topP state variable (default 0.9) to src/components/profiling/ProfilingControls.tsx and included it in the request when starting profiling. Added Top P UI control with range slider (0-1, step 0.05) in the Advanced Settings section, placed between Temperature and Max Tokens controls with descriptive labels ("Top P (Nucleus Sampling)") and range indicators ("0 (restrictive)" to "1 (all tokens)"). Now users can control both temperature and top_p parameters from the UI, and these values are properly passed to model.generate() for both streaming and non-streaming generation modes. Verified with npm build (TypeScript compilation successful) and Python syntax check.
Files Modified: backend/main.py, src/types/index.ts, src/components/profiling/ProfilingControls.tsx

[2026-01-15] BUG-026 - COMPLETED
Status: Fixed profiling history view field mapping
Description: The profiling history feature was already fully implemented with backend endpoint (/api/profiling/runs at line 2260 in backend/main.py), frontend components (HistoryBrowser.tsx, RunList.tsx, RunDetail.tsx, CompareView.tsx), and integration in EnergyProfilerPanel.tsx with History & Analysis tab. However, there was a field name mismatch between backend and frontend: backend returned 'run_id' but frontend ProfilingRun interface expected 'id', and backend returned 'input_token_count'/'output_token_count' but frontend expected 'input_tokens'/'output_tokens' as primary fields. Fixed by updating backend/main.py /api/profiling/runs endpoint to return both naming conventions: added 'id' field (mapped from run_id) for frontend compatibility while keeping 'run_id' for backward compatibility, and added 'input_tokens'/'output_tokens' fields (mapped from token counts) while keeping original field names as aliases. Applied fix to both main response path and fallback path when summary is not available. Users can now view list of past profiling runs with search, filters, sorting, and pagination, drill down into detailed metrics for each run, and compare multiple runs side-by-side.
Files Modified: backend/main.py, plans/prd2.json

[2026-01-15] BUG-028 - COMPLETED
Status: Added GPU power tracking for NVIDIA GPUs
Description: Added support for NVIDIA GPU power tracking to enable profiling on systems with NVIDIA GPUs. Created NvidiaPowerMonitor class in backend/profiling/power_monitor.py that uses nvidia-smi command to query GPU power consumption (nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits). The class implements the same interface as PowerMonitor (is_available, start, stop, get_samples, set_phase, measure_idle_baseline, etc.) for consistent usage across platforms. Added platform detection via Python's platform module. Created create_power_monitor() factory function that: (1) first tries PowerMonitor for macOS Apple Silicon systems with powermetrics, (2) falls back to NvidiaPowerMonitor for systems with NVIDIA GPUs and nvidia-smi, (3) raises RuntimeError if neither is available. NvidiaPowerMonitor samples GPU power at specified intervals (default 100ms), converts from watts to milliwatts, and creates PowerSample objects with gpu_power_mw populated (cpu_power_mw, ane_power_mw, dram_power_mw set to 0). Supports phase tracking, peak power detection, idle baseline measurement, and power sample callbacks just like the original PowerMonitor. On NVIDIA systems, GPU power is now tracked accurately via nvidia-smi, enabling energy profiling on a wider range of hardware platforms beyond Apple Silicon.
Files Modified: backend/profiling/power_monitor.py

[2026-01-15] BUG-029 - COMPLETED
Status: Fixed sampling interval not being respected
Description: Fixed PowerMonitor and NvidiaPowerMonitor to properly track and warn about sample rate deviations. Added sample rate tracking fields (_last_sample_rate_check, _sample_rate_check_interval=5.0s, _sample_count_at_last_check) to both monitor classes. Implemented _check_sample_rate() method that periodically (every 5 seconds) calculates the actual sample rate (samples collected / time elapsed) and compares it to the expected rate (1000ms / sample_interval_ms). Logs a warning if the actual rate deviates by more than 20% from the expected rate, showing both expected and actual rates plus deviation percentage. Integrated the check into the sampling loop of both PowerMonitor (after each plist sample is parsed) and NvidiaPowerMonitor (after each nvidia-smi sample is collected). Reset tracking variables when monitoring starts. This fix enables monitoring of sample rate consistency during profiling and alerts users if powermetrics or nvidia-smi sampling rate varies significantly from the requested interval due to system load or tool limitations. Sample rate logs provide visibility into profiling accuracy and help identify when energy calculations may be affected by inconsistent sampling.
Files Modified: backend/profiling/power_monitor.py

[2026-01-15] BUG-032 - COMPLETED
Status: Added KV cache memory tracking
Description: Fixed missing KV cache size measurement and storage in profiling runs. Added kv_cache_size_mb and context_length fields to ProfilingSession dataclass in backend/profiling/pipeline_profiler.py. Implemented KV cache calculation in backend/main.py after generation completes. The calculation estimates KV cache size using the formula: num_layers * 2 (keys + values) * total_seq_len * num_kv_heads * head_dim * dtype_size, where total_seq_len is input_token_count + output_token_count. Extracts model architecture parameters (num_hidden_layers, hidden_size, num_attention_heads, num_key_value_heads) from model.config. Handles GQA (Grouped Query Attention) models by using num_key_value_heads if available. Determines dtype size from model.dtype (2 bytes for FP16/BF16, 4 bytes for FP32). Converts result to megabytes and stores in session.kv_cache_size_mb along with session.context_length. Updated pipeline_profiler.py save_session() to pass kv_cache_size_mb and context_length to database.update_run_metrics(). Added robust error handling to log warnings and set fields to None if model config attributes are missing. Profiling runs now have kv_cache_size_mb populated with accurate estimated values for memory usage analysis, enabling users to understand KV cache memory pressure and context length effects on energy consumption.
Files Modified: backend/main.py, backend/profiling/pipeline_profiler.py

[2026-01-15] BUG-033 - COMPLETED
Status: Fixed model features extraction
Description: Fixed missing model feature extraction and storage in profiling runs. The backend already had a comprehensive extract_model_features() function in backend/profiling/model_features.py that extracts architectural features like num_layers, hidden_size, intermediate_size, num_attention_heads, num_key_value_heads, total_params, attention_mechanism (MHA/GQA/MQA), is_moe, num_experts, num_active_experts, architecture_type, precision, and quantization_method. However, this function was not being called during profiling, resulting in null values in the database. Fixed by: (1) Adding model feature fields to ProfilingSession dataclass in backend/profiling/pipeline_profiler.py, (2) Adding corresponding parameters to database.py update_run_metrics() function with proper UPDATE statements, (3) Importing extract_model_features in backend/main.py and calling it after model is loaded and moved to device, (4) Storing extracted features in session object for all 11 feature fields, (5) Passing these fields from session to database.update_run_metrics() in pipeline_profiler.py. Added error handling to log warnings if feature extraction fails and continue with null values. Profiling runs now have all model feature columns populated with accurate values, enabling architecture-based energy analysis, model comparison, and correlation studies between model structure and energy consumption.
Files Modified: backend/profiling/pipeline_profiler.py, backend/profiling/database.py, backend/main.py

[2026-01-15] BUG-030 - COMPLETED
Status: Added batch size support in profiling
Description: Added comprehensive batch size support to enable throughput profiling with different batch sizes. Added batch_size field (default 1) to backend/main.py ProfiledGenerateRequest Pydantic model. Modified tokenization in profiled_generate endpoint to duplicate input prompt for batch processing (prompts = [request.prompt] * request.batch_size). Added batch_size parameter to ProfilingSession dataclass and InferencePipelineProfiler.run() method signature in backend/profiling/pipeline_profiler.py. Updated both streaming and non-streaming generation paths to handle batch processing: streaming path comments note TextIteratorStreamer processes first sequence in batch and multiplies output_token_count by batch_size; non-streaming path iterates through all batch sequences, processes each generated_ids, accumulates total_tokens_in_batch, and joins outputs with separator. Updated response handling to join batch outputs with "\n---\n" separator when batch_size > 1. Updated KV cache calculation to multiply by batch_size (kv_cache_size_bytes = ... * request.batch_size) since cache grows linearly with batch size. Updated database.create_run() call in pipeline_profiler.py to pass batch_size from session. Users can now profile with batch_size > 1 for throughput analysis, calculating tokens/second across entire batch. Throughput metrics scale appropriately showing the performance benefits of batch processing. Database stores batch_size for all profiling runs, enabling comparison of energy efficiency and throughput across different batch sizes.
Files Modified: backend/main.py, backend/profiling/pipeline_profiler.py

[2026-01-15] BUG-036 - COMPLETED
Status: Added profiling cancellation support
Description: Fixed the lack of profiling cancellation support by implementing a comprehensive cancellation mechanism. Added profiling_state dictionary in backend/main.py with is_running and should_cancel flags to track profiling status. Created /api/profiling/cancel POST endpoint that sets the should_cancel flag and broadcasts cancellation event to frontend via WebSocket. Modified profiled_generate endpoint to: (1) check if another session is running and return 429 error if so, (2) set is_running=True and should_cancel=False at start, (3) clear both flags on successful completion or error, (4) check should_cancel flag in streaming generation loop and break if set, (5) check should_cancel flag before non-streaming generation and raise exception if set. Updated frontend src/components/profiling/ProfilingContext.tsx stopProfiling function to call the new cancel endpoint before clearing local state. Added API_BASE_URL import to ProfilingContext. The existing "Stop Profiling" button in ProfilingControls.tsx now properly cancels backend profiling. Users can now cancel long-running profiling sessions mid-generation without restarting the server. Cleanup happens properly via the InferencePipelineProfiler context manager (power monitor stop, layer hooks detach, deep profiler unpatch). Frontend receives cancellation notification via WebSocket error event.
Files Modified: backend/main.py, src/components/profiling/ProfilingContext.tsx

[2026-01-15] BUG-034 - COMPLETED
Status: Added WebSocket heartbeat for connection health
Description: Fixed WebSocket connections silently disconnecting during long idle periods by implementing a comprehensive ping/pong heartbeat mechanism. Enhanced frontend ProfilingWebSocketManager class in src/lib/profilingWebsocket.ts by adding heartbeat tracking fields (pingInterval, pongTimeout, lastPongTime) and configurable constants (PING_INTERVAL_MS=30s, PONG_TIMEOUT_MS=60s). Implemented startHeartbeat() method that sends ping messages every 30 seconds via JSON message {type: 'ping'} and checks if pong was received within the last 60 seconds - if not, triggers reconnection. Implemented stopHeartbeat() method to clean up timers on disconnect. Integrated heartbeat lifecycle: startHeartbeat() called on WebSocket open, stopHeartbeat() called on close and disconnect. Updated onmessage handler to detect pong responses and update lastPongTime timestamp. Enhanced backend WebSocket endpoint in backend/main.py to handle ping messages by immediately responding with pong message {type: 'pong', timestamp}. WebSocket now stays connected during long idle periods with active health monitoring. Silent disconnections are detected within 60 seconds and trigger automatic reconnection with exponential backoff. Connection health is continuously monitored, providing reliable real-time communication between frontend and backend during extended profiling sessions.
Files Modified: src/lib/profilingWebsocket.ts, backend/main.py

[2026-01-15] BUG-043 - COMPLETED
Status: Fixed power baseline measurement timing
Description: Fixed idle baseline power measurement to reflect true system idle state by adding a 3-second settling period after model loading. The issue was that model loading causes significant power spikes (GPU/MPS activation, memory transfers), and measuring baseline immediately after model.to(device) captured the tail of this activity rather than true idle power. Added asyncio.sleep(3.0) in backend/main.py after model is moved to device, allowing GPU clocks to ramp down, memory transfers to complete, and system thermals to stabilize. Added informative WebSocket broadcast with status='settling' to notify frontend that system is stabilizing. Enhanced logging to track settling phase. The baseline measurement (measure_idle_baseline) is then called after this settling period by the InferencePipelineProfiler context manager, which sleeps for an additional 2 seconds to collect stable idle power samples. Total delay before inference: 3s settling + 2s baseline = 5s, ensuring baseline accurately reflects idle power with model in memory. This provides accurate delta calculations for active inference power consumption and improves energy measurement reliability.
Files Modified: backend/main.py

[2026-01-15] BUG-044 - COMPLETED
Status: Added EOS token handling for different tokenizers
Description: Fixed generation stop condition to work correctly for all tokenizers by adding proper EOS token ID handling. Different tokenizers use different conventions - some have single eos_token_id, others have lists of multiple EOS tokens. Modified backend/main.py profiled_generate endpoint to add comprehensive EOS token preparation logic before both streaming and non-streaming generation. The fix: (1) checks if tokenizer.eos_token_id exists and is not None, (2) handles both single token ID and list of token IDs by normalizing to list format, (3) includes pad_token_id as an additional stopping token if it differs from EOS token (common in some tokenizers), (4) adds eos_token_id parameter to generation_kwargs for both streaming and non-streaming paths, (5) excludes EOS tokens from token count in non-streaming path by iterating through generated_ids and filtering out tokens matching eos_tokens list. Now generation properly stops at EOS token for all tokenizer types. Token statistics exclude EOS tokens for accurate metrics. Model.generate() receives proper eos_token_id parameter ensuring consistent stopping behavior. Verified with Python syntax check (py_compile successful).
Files Modified: backend/main.py

[2026-01-15] BUG-047 - COMPLETED
Status: Fixed concurrent profiling request handling (already implemented)
Description: Verified that concurrent profiling request handling is already fully implemented and working correctly. The backend uses a global profiling_state dictionary with is_running and should_cancel flags (lines 115-118 in backend/main.py). The profiled_generate endpoint checks if profiling is already running (line 1805) and returns HTTP 429 "Too Many Requests" with clear error message if so. State management is properly handled: sets is_running=True at start (line 1809), clears it on successful completion (line 2416), and clears it in error handling path (line 2425). The implementation ensures only one profiling request runs at a time, prevents interference between concurrent requests, and provides clear feedback to users. Also includes cancellation support via /api/profiling/cancel endpoint (lines 2445-2472) that sets should_cancel flag and broadcasts cancellation event. Concurrent requests are now handled gracefully - second request receives 429 error with message to wait for current session to complete or cancel it.
Files Modified: None (already implemented)

[2026-01-15] BUG-045 - COMPLETED
Status: Fixed attention mask handling for padded inputs (already implemented)
Description: Verified that attention mask handling for padded inputs is already correctly implemented in backend/main.py. The tokenization code (line 2118) uses tokenizer(prompts, return_tensors="pt", padding=True) which automatically creates attention_mask tensor where 1 indicates real tokens and 0 indicates padding. All inputs including attention_mask are moved to the correct device (line 2123): inputs = {k: v.to(device) for k, v in inputs.items()}. The attention_mask is properly passed to model.generate() in both streaming (line 2155) and non-streaming (line 2256) generation paths. This follows PyTorch/HuggingFace best practices where the attention mask ensures models only attend to real tokens and ignore padding positions. The implementation is complete and correct - padded inputs are handled properly, preventing models from attending to padding tokens during inference. No code changes needed.
Files Modified: None (already implemented)

[2026-01-15] BUG-049 - COMPLETED
Status: Fixed thread safety in LayerProfiler (already implemented)
Description: Verified that LayerProfiler is already fully thread-safe through comprehensive implementation in backend/profiling/layer_profiler.py. The class uses threading.local() (line 88) to provide thread-local storage for timings, ensuring each thread has its own isolated timings list. Added threading.Lock() (line 91) for protecting operations that need atomicity. Thread-local storage is accessed via _get_thread_timings() method (lines 138-147) which creates a separate timings list for each thread. Hook callbacks (line 300-301) store timings in thread-local storage, preventing data races when multiple threads execute forward passes concurrently. Public methods use lock protection: get_timings() (line 322-323) acquires lock when copying timings to prevent mid-copy modifications, reset() (line 331-333) uses lock when clearing timings, and detach() (line 355-357) uses lock during cleanup. This dual approach (thread-local storage + locks) ensures no data races under concurrent access while maintaining good performance. The implementation follows best practices for thread-safe PyTorch hook callbacks.
Files Modified: None (already implemented)

[2026-01-15] BUG-035 - COMPLETED
Status: Fixed memory leak in power sample accumulation
Description: Fixed unbounded memory growth in PowerMonitor and NvidiaPowerMonitor classes by implementing sample limit with automatic pruning. Added max_samples parameter (default 10000) to both PowerMonitor and NvidiaPowerMonitor __init__ methods. When sample count reaches max_samples limit, the oldest 10% of samples are automatically removed to prevent unbounded memory accumulation. Added _sample_limit_warned flag to both classes to log a one-time warning when sample count reaches 90% of the limit, alerting users before pruning begins. Implemented pruning logic in both _sampling_loop methods: checks len(self._samples) after each append, warns at 90% threshold, and removes oldest samples when limit is reached. Updated create_power_monitor() factory function to accept and pass max_samples parameter to both monitor types. With default settings (100ms interval, 10000 samples), memory usage caps at approximately 10 minutes of samples before pruning. For longer profiling runs, samples are continuously pruned maintaining a sliding window of the most recent 10000 samples. This prevents memory issues during extended profiling sessions while ensuring samples are saved to database before pruning. Users can increase max_samples if needed for longer retention windows.
Files Modified: backend/profiling/power_monitor.py

