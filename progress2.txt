# Bug Fix Progress Log
Created: Wed Jan 14 22:59:55 +04 2026

[2026-01-14] BUG-001 - COMPLETED
Status: Fixed async event loop error in power sample callback
Description: Fixed 'no running event loop' errors in WebSocket callbacks by capturing the main event loop at the start of profiled_generate() and replacing all asyncio.create_task() calls with asyncio.run_coroutine_threadsafe() to safely broadcast messages from background threads. Updated 6 callbacks: stream_power_sample, stream_section_event, stream_token_complete, stream_layer_metrics, stream_component_metrics, and stream_inference_complete.
Files Modified: backend/main.py

[2026-01-14] BUG-002 - COMPLETED
Status: Fixed StableLM model compatibility with TextIteratorStreamer
Description: Added StableLM architecture detection in model_detector.py with _is_stablelm_structure() and _detect_stablelm() methods. Created is_streaming_compatible() function to identify models incompatible with TextIteratorStreamer. Modified backend/main.py profiled_generate endpoint to check streaming compatibility and use non-streaming generation (use_cache=False) for incompatible models like StableLM. This prevents crashes from KV cache issues while still allowing profiling to complete successfully with appropriate warnings.
Files Modified: backend/profiling/model_detector.py, backend/main.py

[2026-01-14] BUG-003 - COMPLETED
Status: Fixed section event callbacks async issue
Description: All callback functions (stream_section_event, stream_token_complete, stream_layer_metrics, stream_component_metrics, stream_inference_complete) were already fixed as part of BUG-001. They all use asyncio.run_coroutine_threadsafe() with main_loop for thread-safe WebSocket broadcasting. No additional changes needed.
Files Modified: backend/main.py (already fixed in BUG-001)

[2026-01-14] BUG-004 - COMPLETED
Status: Fixed token timing measurement
Description: Fixed token duration measurement which was timing the append() operation (~0ms) instead of actual token generation time. Implemented proper timing by tracking time between consecutive token arrivals from the streamer. Added last_token_time variable to measure duration as (current_token_time - last_token_time). For the first token, uses a reasonable default of 50ms. This fix ensures accurate token duration values (10-100ms per token) and correct energy calculations.
Files Modified: backend/main.py

[2026-01-14] BUG-005 - COMPLETED
Status: Fixed token field name mismatch between backend and frontend
Description: Fixed field name inconsistency where backend sent 'token_index' but frontend expected 'token_position'. Updated three WebSocket callbacks in backend/main.py: stream_token_complete, stream_layer_metrics, and stream_component_metrics to send 'token_position' instead of 'token_index'. This ensures token position displays correctly in all frontend components (TokenGenerationStream hover tooltips, LiveLayerHeatmap, CurrentOperationIndicator, etc.).
Files Modified: backend/main.py

[2026-01-14] BUG-007 - COMPLETED
Status: Added StableLM architecture detection in model_detector.py
Description: StableLM architecture detection was already added as part of BUG-002 fix. The model_detector.py now includes _is_stablelm_structure() method to properly detect StableLM models and _detect_stablelm() method to return correct component paths. StableLM is now correctly identified as 'stablelm' instead of being misidentified as 'llama', ensuring correct profiling behavior.
Files Modified: backend/profiling/model_detector.py (already fixed in BUG-002)

[2026-01-14] BUG-008 - COMPLETED
Status: Fixed energy calculation using near-zero duration
Description: Energy calculation was automatically fixed by BUG-004. Once token timing measurement was corrected to track actual token generation time (10-100ms per token) instead of append() operation time (~0ms), the energy calculation (current_power_mw * token_duration_ms / 1000.0) now produces realistic token energy values (0.1-10 mJ per token depending on model and hardware).
Files Modified: backend/main.py (already fixed in BUG-004)

[2026-01-14] BUG-009 - COMPLETED
Status: Added model streaming compatibility check
Description: Streaming compatibility check was already added as part of BUG-002 fix. The is_streaming_compatible() function in model_detector.py checks if a model supports TextIteratorStreamer by detecting known incompatible architectures (like StableLM). The profiled_generate endpoint in backend/main.py checks compatibility before using streaming and falls back to non-streaming generation for incompatible models.
Files Modified: backend/profiling/model_detector.py, backend/main.py (already fixed in BUG-002)

[2026-01-14] BUG-016 - COMPLETED
Status: Added Gemma 3 architecture detection
Description: Gemma architecture detection was already implemented in model_detector.py. The detector includes _is_gemma_structure() method and _detect_gemma() method that properly identifies Gemma models (including Gemma 2 and Gemma 3) by checking for model_type == 'gemma' or 'gemma2'. Returns correct component paths with RMSNorm and standard Llama-like structure.
Files Modified: backend/profiling/model_detector.py (already implemented)

[2026-01-14] BUG-017 - COMPLETED
Status: Fixed layer profiler hooks for different architectures
Description: Enhanced LayerProfiler in layer_profiler.py to better support multiple architectures. Added registration statistics tracking to count successful hook registrations per component type. Improved logging to show architecture name, registration summary, and warnings when major components aren't found. Added diagnostic logging for first layer structure. Enhanced error messages to only log on first layer (avoid spam) and include architecture information. Added get_registration_summary() method for external code to check registration success. The LayerProfiler already uses ModelArchitectureDetector dynamically, so it now properly handles all supported architectures (Llama, Mistral, Phi, Qwen, Gemma, StableLM) with better diagnostics when hooks fail to attach.
Files Modified: backend/profiling/layer_profiler.py

[2026-01-14] BUG-006 - COMPLETED
Status: Fixed WebSocket URL hardcoding in frontend
Description: Fixed hardcoded localhost:8000 URLs in frontend by creating centralized configuration. Created src/lib/config.ts that exports API_BASE_URL and getWebSocketUrl() function, both of which check environment variables (NEXT_PUBLIC_API_URL and NEXT_PUBLIC_WS_URL) before falling back to localhost:8000. Updated src/lib/api.ts to import API_BASE_URL from config. Updated src/components/profiling/ProfilingControls.tsx to use API_BASE_URL for model fetching instead of hardcoded URL. Updated src/lib/websocket.ts and src/lib/profilingWebsocket.ts to use getWebSocketUrl() for WebSocket connections. The WebSocket URL is automatically derived from the HTTP API URL (http:// -> ws://, https:// -> wss://), but can be overridden with NEXT_PUBLIC_WS_URL. Frontend now supports deployment to any URL by setting NEXT_PUBLIC_API_URL environment variable.
Files Modified: src/lib/config.ts (new), src/lib/api.ts, src/components/profiling/ProfilingControls.tsx, src/lib/websocket.ts, src/lib/profilingWebsocket.ts

[2026-01-14] BUG-010 - COMPLETED
Status: Fixed profiling cleanup on error
Description: Fixed potential cleanup issues when profiling fails mid-execution. The InferencePipelineProfiler context manager already had proper cleanup in its finally block (lines 339-387 in pipeline_profiler.py), which handles stopping power monitoring, detaching layer profiler hooks, unpatching deep profiler, and saving data to database. However, backend/main.py had redundant cleanup code (lines 2157-2168) that was removed since the context manager handles it automatically. Added proper error handling in the except block to send profiling_error WebSocket events to the frontend when exceptions occur, using the existing ProfilingMessageType.ERROR message type. This ensures the frontend is notified of errors and all resources (PowerMonitor subprocess, layer hooks, deep profiler patches) are properly cleaned up even when profiling fails. Added import time statement to support error timestamp generation.
Files Modified: backend/main.py

